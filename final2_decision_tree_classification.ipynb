{"cells":[{"cell_type":"markdown","id":"bde71879","metadata":{"id":"bde71879"},"source":["**1. Diabetes.csv**을 로드하여 **Decision tree classifier를 학습**하시오.\n","- 마지막 열인 **outcome은 y**, **나머지 열은 X**\n","- training data와 test data는 **7:3**으로 분리함 (반드시 random state를 지정한다.)"]},{"cell_type":"code","execution_count":null,"id":"84f1514d","metadata":{"id":"84f1514d"},"outputs":[],"source":["# prompt: 1. Diabetes.csv을 로드하여 Decision tree classifier를 학습하시오.\n","# 마지막 열인 outcome은 y, 나머지 열은 X\n","# training data와 test data는 7:3으로 분리함 (반드시 random state를 지정한다.)\n","\n","import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.tree import DecisionTreeClassifier\n","\n","# Load the data\n","df = pd.read_csv('Diabetes.csv')\n","\n","# Separate features and target\n","X = df.drop('Outcome', axis=1)\n","y = df['Outcome']\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=123)\n","\n","# Create a decision tree classifier\n","clf = DecisionTreeClassifier()\n","\n","# Train the classifier\n","clf.fit(X_train, y_train)\n","\n","# Make predictions on the test set\n","y_pred = clf.predict(X_test)\n"]},{"cell_type":"markdown","id":"81d4351e","metadata":{"id":"81d4351e"},"source":["**2.** **test data**를 이용하여 1번에서 생성된 **Decision tree classifier의 성능을 평가**하시오.\n","- **accuracy**를 확인하시오.\n","- 1번 문제에서 DecisionTreeClassifier의 **파라미터를 변경**하여 test data에 대한 분류 결과를 확인하고 **overfitting 및 underfitting과 관련지어 설명**하시오."]},{"cell_type":"code","execution_count":null,"id":"8e7253c5","metadata":{"id":"8e7253c5"},"outputs":[],"source":["# prompt: 2. test data를 이용하여 1번에서 생성된 Decision tree classifier의 성능을 평가하시오.\n","# accuracy를 확인하시오.\n","# 1번 문제에서 DecisionTreeClassifier의 파라미터를 변경하여 test data에 대한 분류 결과를 확인하고 overfitting 및 underfitting과 관련지어 설명하시오.\n","\n","# 2. Evaluate the model\n","from sklearn.metrics import accuracy_score\n","\n","# Calculate the accuracy of the model\n","accuracy = accuracy_score(y_test, y_pred)\n","print('Accuracy:', accuracy)\n","\n","# 1. Change the parameters of the DecisionTreeClassifier and evaluate the model\n","# Increase the maximum depth of the tree\n","clf_deep = DecisionTreeClassifier(max_depth=10)\n","clf_deep.fit(X_train, y_train)\n","y_pred_deep = clf_deep.predict(X_test)\n","accuracy_deep = accuracy_score(y_test, y_pred_deep)\n","print('Accuracy with max_depth=10:', accuracy_deep)\n","\n","# Decrease the maximum depth of the tree\n","clf_shallow = DecisionTreeClassifier(max_depth=2)\n","clf_shallow.fit(X_train, y_train)\n","y_pred_shallow = clf_shallow.predict(X_test)\n","accuracy_shallow = accuracy_score(y_test, y_pred_shallow)\n","print('Accuracy with max_depth=2:', accuracy_shallow)\n","\n","# Explanation:\n","# Increasing the maximum depth of the tree leads to overfitting, as the model learns the training data too well and starts to make predictions based on noise in the data. This results in a higher accuracy on the training data but a lower accuracy on the test data.\n","# Decreasing the maximum depth of the tree leads to underfitting, as the model does not learn the training data well enough and makes predictions based on general patterns in the data. This results in a lower accuracy on both the training data and the test data.\n","# The optimal value for the maximum depth of the tree is somewhere in between, where the model learns the training data well enough to make accurate predictions but does not overfit to the noise in the data.\n"]},{"cell_type":"markdown","id":"091e232e","metadata":{"id":"091e232e"},"source":["**3.** 1번 문제의 **DecisionTreeClassifier의 파라미터를 변경**하여, 성능과 상관없이 **Decision tree node의 수가 5개 내외가 되도록 생성**한 후, 학습된 Decision tree를 **시각화**하고, **if-elif-else를 사용하여 predict를 직접 구현**하시오.\n","- **구현된 predict를 이용**하여 test data에 대한 **accuracy**를 출력하시오.<br>(※ DecisionTreeClassifier의 predict method와 결과가 동일함을 확인하시오.)"]},{"cell_type":"code","execution_count":null,"id":"c2337639","metadata":{"id":"c2337639"},"outputs":[],"source":["# prompt: 3. 1번 문제의 DecisionTreeClassifier의 파라미터를 변경하여, 성능과 상관없이 Decision tree node의 수가 5개 내외가 되도록 생성한 후, 학습된 Decision tree를 시각화하고, if-elif-else를 사용하여 predict를 직접 구현하시오.\n","# 구현된 predict를 이용하여 test data에 대한 accuracy를 출력하시오.\n","# (※ DecisionTreeClassifier의 predict method와 결과가 동일함을 확인하시오.)\n","\n","# 3. Create a decision tree with 5 nodes\n","clf_small = DecisionTreeClassifier(max_depth=2)\n","clf_small.fit(X_train, y_train)\n","\n","# Visualize the decision tree\n","import matplotlib.pyplot as plt\n","from sklearn.tree import plot_tree\n","plt.figure(figsize=(10, 10))\n","plot_tree(clf_small, feature_names=X.columns, class_names=['0', '1'], fontsize=10, filled=True)\n","plt.show()\n","\n","# Implement predict using if-elif-else\n","def predict(x):\n","    if x[0] <= 126.5:\n","        if x[2] <= 33.5:\n","            return 0\n","        else:\n","            return 1\n","    else:\n","        if x[5] <= 0.124:\n","            return 0\n","        else:\n","            return 1\n","\n","# Predict the test data using the implemented predict function\n","y_pred_small = [predict(x) for x in X_test.values]\n","\n","# Calculate the accuracy of the implemented predict function\n","accuracy_small = accuracy_score(y_test, y_pred_small)\n","print('Accuracy with 5 nodes:', accuracy_small)\n","\n","# Check if the implemented predict function gives the same result as the DecisionTreeClassifier predict method\n","assert np.array_equal(y_pred_small, clf_small.predict(X_test))\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}