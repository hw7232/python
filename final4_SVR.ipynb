{"cells":[{"cell_type":"markdown","id":"307853b4","metadata":{"id":"307853b4"},"source":["**1.** **SVR**과 **ridge regression과의 차이점**을 설명하시오."]},{"cell_type":"code","execution_count":null,"id":"793b4324","metadata":{"id":"793b4324"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"11654775","metadata":{"id":"11654775"},"source":["**2.** **diamonds.csv** 파일을 로드하여, **RBF kernel SVR을 학습**하시오.\n","- diamonds dataset은 캐럿, 색상 등의 정보로 다이아몬드의 가격을 예측하기 위한 dataset임\n","- 마지막 열인 **price는 y, 나머지 열 중 3개 이상의 열을 X**로 선정하되, **문자열인 열을 모두 X에 포함할 경우, 가산점** 있음<br>\n","cut, color, clarity value는 모두 문자열인 범주형 데이터로 Encoding 과정이 필요함<br>\n","(※ value가 문자열인 column만 추출하여 encoding → 문자열인 column 삭제 → One hot encoding 결과 결합)\n","- training data와 test data는 **8:2**로 분리함\n","- **coef0와 gamma는 default**로 설정하시오.\n","- SVR 또는 SVM에서 활용하는 **kernel trick에 대해 설명**하시오."]},{"cell_type":"markdown","id":"3b2be4c4","metadata":{"id":"3b2be4c4"},"source":["- `np.del(ndarray,[a,b,c],axis=1)`: ndarray의 index a,b,c 열을 삭제함 (axis=0일 경우 행 삭제)\n","- `np.concatenate((ndarray1,ndarray2),axis=1)`: ndarray1과 ndarray2를 좌우로 결합"]},{"cell_type":"code","execution_count":null,"id":"61f943f9","metadata":{"id":"61f943f9"},"outputs":[],"source":["# prompt: 2. diamonds.csv 파일을 로드하여, RBF kernel SVR을 학습하시오.\n","# diamonds dataset은 캐럿, 색상 등의 정보로 다이아몬드의 가격을 예측하기 위한 dataset임\n","# 마지막 열인 price는 y, 나머지 열 중 3개 이상의 열을 X로 선정하되, 문자열인 열을 모두 X에 포함할 경우, 가산점 있음\n","# cut, color, clarity value는 모두 문자열인 범주형 데이터로 Encoding 과정이 필요함\n","# (※ value가 문자열인 column만 추출하여 encoding → 문자열인 column 삭제 → One hot encoding 결과 결합)\n","# training data와 test data는 8:2로 분리함\n","# coef0와 gamma는 default로 설정하시오.\n","# SVR 또는 SVM에\n","\n","import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import OneHotEncoder\n","from sklearn.svm import SVR\n","\n","# Load the diamonds dataset\n","diamonds = pd.read_csv(\"diamonds.csv\")\n","\n","# Select features\n","X = diamonds[['carat', 'cut', 'color', 'clarity']]\n","\n","# Encode categorical features\n","categorical_cols = ['cut', 'color', 'clarity']\n","onehot_encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')\n","encoded_cols = pd.DataFrame(onehot_encoder.fit_transform(diamonds[categorical_cols]))\n","encoded_cols.index = diamonds.index\n","\n","# Drop categorical columns\n","X.drop(categorical_cols, axis=1, inplace=True)\n","\n","# Concatenate encoded columns\n","X = pd.concat([X, encoded_cols], axis=1)\n","\n","# Select target variable\n","y = diamonds['price']\n","\n","# Split data into training and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Create and train the SVR model\n","model = SVR(kernel='rbf', coef0=0, gamma='scale')\n","model.fit(X_train, y_train)\n","\n","# Print the model parameters\n","print(\"Model parameters:\")\n","print(\"- Kernel:\", model.kernel)\n","print(\"- C:\", model.C)\n","print(\"- Gamma:\", model.gamma)\n","print(\"- Epsilon:\", model.epsilon)\n","\n","# Evaluate the model on the test set\n","score = model.score(X_test, y_test)\n","print(\"Test set score:\", score)\n"]},{"cell_type":"markdown","id":"426cf42b","metadata":{"id":"426cf42b"},"source":["**3.** test dataset을 이용하여 2번 **RBF kernel SVR의 성능을 평가**하시오.\n","- $R^{2}$(`r2_score`)로 성능을 보이시오.\n","- linear.svm.SVR의 parameter 중 **epsilon은 어떠한 값을 의미**하는지 설명하고, 해당 parameter를 **증가시켰을 때, 생성된 support vector의 수가 어떻게 변화**하는지, **변화하는 근거**에 대해 간단히 설명하시오."]},{"cell_type":"code","execution_count":null,"id":"707b8935","metadata":{"id":"707b8935"},"outputs":[],"source":["# prompt: 3. test dataset을 이용하여 2번 RBF kernel SVR의 성능을 평가하시오.\n","# R2 (r2_score)로 성능을 보이시오.\n","# linear.svm.SVR의 parameter 중 epsilon은 어떠한 값을 의미하는지 설명하고, 해당 parameter를 증가시켰을 때, 생성된 support vector의 수가 어떻게 변화하는지, 변화하는 근거에 대해 간단히 설명하시오.\n","\n","from sklearn.metrics import r2_score\n","\n","# Evaluate the model on the test set\n","y_pred = model.predict(X_test)\n","r2 = r2_score(y_test, y_pred)\n","print(\"Test set R2 score:\", r2)\n","\n","# Explanation of epsilon\n","epsilon = model.epsilon\n","print(\"Epsilon:\", epsilon)\n","print(\"Epsilon is the margin of tolerance around the hyperplane within which no penalty is incurred.\")\n","\n","# Effect of increasing epsilon on the number of support vectors\n","model_high_epsilon = SVR(kernel='rbf', coef0=0, gamma='scale', epsilon=2*epsilon)\n","model_high_epsilon.fit(X_train, y_train)\n","num_support_vectors_high_epsilon = len(model_high_epsilon.support_)\n","\n","print(\"Number of support vectors with high epsilon:\", num_support_vectors_high_epsilon)\n","\n","# Explanation of the change in the number of support vectors\n","print(\"Increasing epsilon relaxes the margin, allowing more data points to be classified as support vectors.\")\n","\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}